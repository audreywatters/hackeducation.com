---
layout: post
title: "'Technology-Enhanced Retention' and Other Ed-Tech Interventions"
date: 2017-04-05 07:01:00 +0000
tags:
image: https://s3.amazonaws.com/hackedu/2017-04-05-wall.jpg
imagecredits: https://www.flickr.com/photos/kingy/3623648642
---
<p><em>These remarks were given yesterday at Coventry University as part of my visiting fellowship at the Disruptive Media Learning Lab. I took part in a panel on "Technology-Enhanced Student Attainment and Retention" with Daniel Burgos from the International University of La Rioja and Lynn Clouder from Coventry University</em></p>

<p>As I prepped for my remarks here, I did stop to think a bit about whether I&#8217;m the right respondent &#8211; am I a social scientist? My formal academic training was really much more in the humanities &#8211; I dropped out of a PhD program in Comparative Literature. I do have a graduate degree in Folklore Studies, which is a kin of anthropology and is a field that is a bit of both, I suppose: social sciences and the humanities. I do consider myself, in some ways, an ethnographer. What I am not &#8211; not really or not particularly well &#8211; is a <em>quantitative</em> researcher. Or at least, I&#8217;ve never taken a class in educational research methods, and it&#8217;s been about 20 years since I took a class in statistics. I have only the vaguest recollection of what p values are and why they&#8217;re significant (I think that&#8217;s a bit of word play. But I am not certain).</p>

<p>What I do have, with full confidence, is a solid rolodex. I have friends who do education research and run regression tables for a living. And when press releases about studies on various education technologies cross my desk, I often ask for their help in deciphering the findings.</p>

<p>That&#8217;s what journalists should do instead of relying on the PR or on the abstracts from journal articles &#8211; which in fairness, if you don&#8217;t have access to a research library is sometimes all you can read. That&#8217;s what academics and administrators should do instead of relying on the PR or on the salespeople who offer you freebies at conferences.</p>

<p>But let me pause for a minute and restate that: <em>when press releases about studies on various education software cross my desk</em>&#8230; Press releases, my inbox is full of them &#8211; sometimes from universities, more often from the software makers themselves. Salespeople, the industry is full of them. There&#8217;s a lot of marketing about educational software. There&#8217;s a lot of hype about educational software. But that&#8217;s not necessarily because there&#8217;s a lot of solid research that demonstrates &#8220;effectiveness&#8221; or (and this is key) a lot of &#8220;good&#8221; ed-tech.</p>

<p>And I&#8217;ll say something that people might find upsetting or offensive: I&#8217;m not sure that &#8220;solid research&#8221; would necessarily impress me. I don&#8217;t actually care about &#8220;assessments&#8221; or &#8220;effectiveness.&#8221; That is, they&#8217;re not interesting to me as a scholar. My concerns about &#8220;what works&#8221; about ed-tech have little to do with whether or not there&#8217;s something we can measure or something we can bottle as an &#8220;outcome&#8221;; indeed, I fear that what we can measure often shapes our discussions of &#8220;effect.&#8221;</p>

<p>What interests me nonetheless are the <em>claims</em> that are made about ed-tech &#8211; what we are told it can do. I listen for these stories and the recurring themes in them because I think they reveal a number of really important things: what we value, <em>who</em> we value in education; how we imagine learning happens; what we think is wrong with the current model of teaching and/or system of education; what we think will fix all this; and so on. </p>

<p>(I use that pronoun &#8220;we&#8221; in its broadest sense. Like &#8220;we all humans.&#8221; I do want us to recognize there are many, many competing values and many, many competing visions for education. And that means there are lots of opinions &#8211; many, many that are grounded in &#8220;research&#8221; and many, many that are peddled by researchers themselves &#8211; about what we should do to make teaching and learning &#8220;better.&#8221;)</p>

<p>Why does attainment matter, for example? To whom does attainment matter? What do we mean by attainment? If it something we can measure? Is it something that education can actually intervene upon? If so, how? If so, in what ways? Why does retention matter? To whom does retention matter? Why? Why do we use words like &#8220;intervention&#8221; to describe our efforts to address &#8220;retention&#8221; or &#8220;attainment&#8221;? </p>

<p>Do we use the word &#8220;intervention&#8221; because it&#8217;s a medical term? A scientific term? Are we diagnosing something about students?</p>

<p>As such, I&#8217;m very interested in the phrase &#8220;technology-enhanced&#8221; in the title of this panel. First of all, I think it does underscore that what we do without technology &#8211; to attain, to retain &#8211; doesn&#8217;t work. (We can ask &#8220;doesn&#8217;t work <em>for whom</em>?) Let&#8217;s consider why not. Is it an institutional issue? A systemic, societal one? Is there something &#8221;wrong&#8220; with students? Do we see this as a human issue? Or, as it&#8217;s technology-enhanced,&#8221; is it an engineering problem?</p>

<p>My concern, I think &#8211; and I repeat this a lot &#8211; is that we have substituted surveillance for care. Our institutions do not care for students. They do not care for faculty. They have not rewarded those in it for their compassion, for their relationships, for their humanity.</p>

<p>Adding a technology layer on top of a dispassionate and exploitative institution does not solve anyone&#8217;s problems. Indeed, it creates new ones. What do we lose, for example, if we more heavily surveil students? What do we lose when we more heavily surveil faculty? The goal with technology-enhanced efforts, I fear, is compliance not compassion and not curiosity. So sure, some &#8220;quantitative metrics&#8221; might tick upward. But at what cost? And at what cost to whom?</p>